{"cells":[{"metadata":{},"cell_type":"markdown","source":"# FastAI Chapter 8 - Collaborative Filtering"},{"metadata":{},"cell_type":"markdown","source":"The chapter talks about methods to predict entries in big matrices of, for instance customer data by predicting latent factors underlying the customers' choice. Hence, a recommendation system. We want to learn which movies are similar to each other and hence which unseen movie a customer might like based on the movies he already liked. Essentially, we want to assign each movie a vector in a continuous vector space. Movies that are closer together in that vector space are considered to be similar based on the input data set.\n\nTake, for instance movie rating from numerous user on IMDB. We know how certain customers rate certain movies. But we don't know which measures the users based that rating on or how they would rate other movies. But we can learn that.\n\nCollaborative Filtering means to randomly initialze a number of latent factors for each user AND each movie, then calculate the DOT PRODUCT of both sets of factors and calculate the loss compared to the existing ratings.\n\nOnce we have learned those latent factors we can predict which movie that user might like."},{"metadata":{},"cell_type":"markdown","source":"The most important concept to understand to solve this task is so called \"Embedding\". \nImagine a list of 100 movies. We could represent each movie with a one-hot vector with a one at the list position of the movie. However, using a large and sparse vector like this is very memory intensive and would slow down everything. Instead we can assign each movie a certain number of randomly initialized features and learn their \"true\" value. "},{"metadata":{},"cell_type":"markdown","source":"The model we are going to use is hence a dot product of embeddings for users and movies. In addition we need a bias which in this example is the underlying quality of the movie. A movie can be \"very action\" and the user can like action movies, but if it's a bad movie the rating is still going to be bad. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#hide\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hide\nfrom fastbook import *","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.collab import *\nfrom fastai.tabular.all import *\npath = untar_data(URLs.ML_100k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings = pd.read_csv(path/'u.data', delimiter='\\t', header=None,\n                      names=['user','movie','rating','timestamp'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"movies = pd.read_csv(path/'u.item',  delimiter='|', encoding='latin-1',\n                     usecols=(0,1), names=('movie','title'), header=None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ratings = ratings.merge(movies)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dls = CollabDataLoaders.from_df(ratings, item_name='title', bs=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_users  = len(dls.classes['user'])\nn_movies = len(dls.classes['title'])\nn_factors = 5\n\nuser_factors = torch.randn(n_users, n_factors)\nmovie_factors = torch.randn(n_movies, n_factors)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DotProductBias(Module):\n    def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)):\n        self.user_factors = Embedding(n_users, n_factors)\n        self.user_bias = Embedding(n_users, 1)\n        self.movie_factors = Embedding(n_movies, n_factors)\n        self.movie_bias = Embedding(n_movies, 1)\n        self.y_range = y_range\n        \n    def forward(self, x):\n        users = self.user_factors(x[:,0])\n        movies = self.movie_factors(x[:,1])\n        res = (users * movies).sum(dim=1, keepdim=True)\n        res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n        return sigmoid_range(res, *self.y_range)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DotProductBias(n_users, n_movies, 50)\nlearn = Learner(dls, model, loss_func=MSELossFlat())\nlearn.fit_one_cycle(5, 5e-3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Weight decay or L2 regularization"},{"metadata":{},"cell_type":"markdown","source":"In order to keep weights small, hence the search space for the SGD relatively smooth without deep valleys we introduce a regularization penalty by adding to the loss function the sum of all the weights squared.\nIn code we simply add a wd parameter. \"wd\" is basiclly lambda for the regularization."},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 5e-3, wd=0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# In short using fastAI"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(5, 5e-3, wd=0.1)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}